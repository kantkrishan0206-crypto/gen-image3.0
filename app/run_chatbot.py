# Licensed under the TENCENT HUNYUAN COMMUNITY LICENSE AGREEMENT (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://github.com/Tencent-Hunyuan/HunyuanImage-3.0/blob/main/LICENSE
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

import argparse
import random
from datetime import datetime
from pathlib import Path
from typing import Optional

import gradio as gr
from gradio import ChatMessage

from app.pipeline import HunyuanImage3AppPipeline
from app.style import load_css
from hunyuan_image_3.system_prompt import t2i_system_prompts

# Global vars
hyi3_pipeline: Optional[HunyuanImage3AppPipeline] = None
image_cache_dir: Optional[Path] = None


def default(val, default_val):
    return val if val is not None else default_val


def load_pipeline(args):
    """ Load the HunyuanImage-3 pipeline """
    global hyi3_pipeline
    hyi3_pipeline = HunyuanImage3AppPipeline(args)
    print("Model and tokenizer loaded.")

    global image_cache_dir
    image_cache_dir = args.image_cache_dir
    if image_cache_dir is not None:
        # Cache image by date
        image_cache_dir = Path(image_cache_dir)
        print("Image cache dir:", image_cache_dir)


def update_history(history, message):
    """ Update chatbot history """
    assert 'text' in message and 'files' in message

    # extra_img_input = preprocess_mask_img(img_input)
    extra_img_input = None
    for x in message["files"]:
        history.append(ChatMessage(role="user", content=gr.Image(x, type="pil", format="png")))
    if message["text"] is not None:
        history.append(ChatMessage(role="user", content=message["text"]))
    if extra_img_input is not None:
        history.append(ChatMessage(role="user", content=gr.Image(extra_img_input, type="pil", format="png")))
    return history, gr.MultimodalTextbox(value=None, interactive=False)


def spinner():
    """ Return a spinner to denote image generation in progress """
    return """<div class="typing-dots">
                <div class="typing-dot"></div>
                <div class="typing-dot"></div>
                <div class="typing-dot"></div>
            </div>"""


def hunyuan_image_3_respond(history, system_prompt,
                            seed, top_k, top_p, temperature, infer_steps, diff_guidance_scale,
                            image_size, bot_task, context_mode,
                            ):
    """
    HunyuanImage-3 response generation function

    Args:
        history (List[Dict[str, str]]): Chat history
        system_prompt (str): System prompt
        seed (int): Random seed. -1 means random seed.
        top_k (float): Top-K for text generation
        top_p (float): Top-P for text generation:
        temperature (float): Temperature for text generation
        infer_steps (int): Diffusion inference steps
        diff_guidance_scale (float): Diffusion guidance scale
        image_size (str): Image size. "auto" or "HxW" or "H:W"
        bot_task (str): Bot task.
            "image": Only generate image. If image_size is "auto", the image ratio token will be predicted at first.
            "auto": Text generation. The model will decide whether to generate text or image.
            "think": Given user inputs, start thinking and then rewrite the prompt for image generation, finally
                        generate image.
            "recaption": Given user inputs, rewrite the prompt for image generation, finally generate image.
        context_mode (str): Context mode. "single_round", "unlimited"
    """
    extra_kwargs = {
        "seed": random.randint(0, 1_000_000) if seed < 0 else seed,
        "top_k": top_k,
        "top_p": top_p,
        "temperature": float(temperature),
        "diff_infer_steps": infer_steps,
        "diff_guidance_scale": diff_guidance_scale,
        "image_size": image_size,
        "bot_task": bot_task,
        "context_mode": context_mode,
        "drop_think": hyi3_pipeline.model.generation_config.drop_think,     # drop think when gen_image
    }
    eos = "<|endoftext|>"

    input_message_list = [message for message in history if message["content"] != ""]
    if system_prompt:
        input_message_list = [dict(
            role="system", content=system_prompt, type="text", content_type='str',
        )] + input_message_list

    current_text_response = ""
    history.append({"role": "assistant", "content": ""})

    for r in hyi3_pipeline.generate(input_message_list, **extra_kwargs):
        if r["type"] == "text" and r["value"] not in (eos, ""):
            current_text_response += r["value"]
            history[-1]["content"] = current_text_response
            yield history

        elif r["type"] == "flag":
            if r["value"] == "image":
                # Add a spinner for image generation
                if current_text_response:
                    yield history
                    current_text_response = ""
                history.append({"role": "assistant", "content": spinner()})
                yield history

        elif r["type"] == "image":
            # Finish current text response
            if current_text_response:
                yield history
                history.append({"role": "assistant", "content": ""})
                current_text_response = ""
            # Remove spinner
            if history[-1]["content"] == spinner():
                history.pop()
            # Append and save image
            history.append({"role": "assistant", "content": gr.Image(r["value"], type="pil", format="png")})
            if image_cache_dir is not None:
                date = datetime.now()
                img_path = image_cache_dir / date.strftime("%Y%m%d") / f"img_{date.strftime('%H%M%S_%f')}.png"
                img_path.parent.mkdir(parents=True, exist_ok=True)
                r["value"].save(img_path)
                print(f"Image saved to {img_path}")
            yield history
            history.append({"role": "assistant", "content": ""})

    if not history[-1]["content"]:
        history.pop()
    yield history


def handle_undo(history, undo_data: gr.UndoData):
    """ Handle undo action """
    return history[:undo_data.index], gr.MultimodalTextbox(value=history[undo_data.index]['content'], interactive=True)


def handle_retry(history, retry_data: gr.RetryData, *args, **kwargs):
    """ Handle retry action """
    new_history = history[:retry_data.index + 1]
    yield from hunyuan_image_3_respond(new_history, *args, **kwargs)


def get_system_prompt(sys_type, bot_task):
    if sys_type == 'None':
        visible = False
        value = ""
    elif sys_type in ['en_vanilla', 'en_recaption', 'en_think_recaption']:
        visible = True
        value = t2i_system_prompts[sys_type][0]
    elif sys_type == "dynamic":
        visible = True
        if bot_task == "think":
            value = t2i_system_prompts["en_think_recaption"][0]
        elif bot_task == "recaption":
            value = t2i_system_prompts["en_recaption"][0]
        elif bot_task == "image":
            value = t2i_system_prompts["en_vanilla"][0].strip("\n")
        else:
            value = ""
    elif sys_type == 'custom':
        visible = True
        value = ""
    else:
        raise NotImplementedError(f"Unsupported system prompt type: {sys_type}")
    return gr.TextArea(value=value, lines=7, max_lines=7, placeholder="Please input system prompt", show_label=False,
                       visible=visible, elem_id="system-prompt")


def create_ui_interface(args):
    gen_config = hyi3_pipeline.model.generation_config
    block = gr.Blocks(fill_height=True, css=load_css())
    with block:
        with gr.Column():
            # ==== Left ====
            #  Sidebar
            with gr.Sidebar(open=args.open_sidebar, width='20%'):
                with gr.Accordion("Image Generation", open=True, visible=True):
                    with gr.Row(elem_id="Image Generation parameter", visible=True):
                        image_size = gr.Dropdown([
                            ("Auto", "auto"),
                            ("1:1", "1024x1024"),
                            ("4:3", "896x1152"),
                            ("3:4", "1152x896"),
                            ("16:9", "768x1280"),
                            ("9:16", "1280x768"),
                            ("21:9", "640x1408"),
                        ], label="Image size", value=args.image_size)
                        seed = gr.Number(
                            label="Seed", minimum=-1, maximum=1_000_000, value=args.seed, step=1, precision=0,
                            min_width=80,
                        )
                        infer_steps = gr.Slider(
                            label="Infer Steps", minimum=1, maximum=200,
                            value=default(args.diff_infer_steps, gen_config.diff_infer_steps), step=1,
                            min_width=200,
                        )
                        diff_guidance_scale = gr.Slider(
                            label="Guidance", minimum=1.0, maximum=16.0,
                            value=default(args.diff_guidance_scale, gen_config.diff_guidance_scale), step=0.5,
                            min_width=200,
                        )
                        use_system_prompt = gr.Dropdown([
                            ("None", 'None'),
                            ("Preset(Dynamic)", "dynamic"),
                            ("Preset(Default)", 'en_vanilla'),
                            ("Preset(Recaption)", 'en_recaption'),
                            ("Preset(Think+Recaption)", 'en_think_recaption'),
                            ("Custom", 'custom'),
                        ], label="System Prompt", value=default(args.use_system_prompt, gen_config.use_system_prompt))
                        bot_task = gr.Dropdown([
                            ("Image", "image"),
                            ("Auto", "auto"),
                            ("Think", "think"),
                            ("Recaption", "recaption"),
                        ], label="Bot Task", value=default(args.bot_task, gen_config.bot_task), min_width=150)
                        context_mode = gr.Dropdown([
                            ("Single Round", "single_round"),
                            ("All", "unlimited"),
                        ], label="Context Mode", value=args.context_mode, min_width=150)
                with gr.Accordion("Text Generation", open=False, visible=True):
                    with gr.Row(elem_id="Text Generation parameter"):
                        top_k = gr.Slider(
                            label="Top-K", minimum=1, maximum=16384,
                            value=default(args.top_k, gen_config.top_k), step=1, min_width=200,
                        )
                        top_p = gr.Slider(
                            label="Top-P", minimum=0.0, maximum=1.0,
                            value=default(args.top_p, gen_config.top_p), step=0.01, min_width=200,
                        )
                        temperature = gr.Slider(
                            label="Temperature", minimum=0.1, maximum=1.0,
                            value=default(args.temperature, gen_config.temperature), step=0.1, min_width=200,
                        )

            # ==== Right ====
            #  System prompt
            accordion = gr.Accordion("System Prompt", open=False)
            with accordion:
                system_prompt = get_system_prompt(
                    default(args.use_system_prompt, gen_config.use_system_prompt),
                    default(args.bot_task, gen_config.bot_task),
                )
            #  Chatbot
            chatbot = gr.Chatbot(
                min_height=500,
                elem_id="chatbot",
                bubble_full_width=False,
                type="messages",
                scale=1,
                avatar_images=('./assets/user.png', './assets/robot.png'),
                allow_tags=["think", "recaption"],
            )
            #  Input text box
            with gr.Row(scale=0):
                chat_input = gr.MultimodalTextbox(
                    interactive=True,
                    file_count="multiple",
                    file_types=["image"],
                    scale=15,
                    placeholder="Enter message or upload file...", show_label=False,
                    max_plain_text_length=65536,
                )

            #  Events
            chatbot.undo(handle_undo, chatbot, [chatbot, chat_input])
            chatbot.retry(
                handle_retry,
                [
                    chatbot, system_prompt,
                    seed, top_k, top_p, temperature, infer_steps, diff_guidance_scale,
                    image_size, bot_task, context_mode,
                ],
                chatbot,
            )

            chat_input.submit(
                update_history,
                [chatbot, chat_input],
                [chatbot, chat_input],
                queue=False,
            ).then(
                hunyuan_image_3_respond,
                [
                    chatbot, system_prompt,
                    seed, top_k, top_p, temperature, infer_steps, diff_guidance_scale,
                    image_size, bot_task, context_mode
                ],
                chatbot
            ).then(
                lambda: gr.MultimodalTextbox(interactive=True), None, [chat_input]
            )

            use_system_prompt.change(fn=get_system_prompt, inputs=[use_system_prompt, bot_task], outputs=system_prompt)
            bot_task.change(fn=get_system_prompt, inputs=[use_system_prompt, bot_task], outputs=system_prompt)

    return block


def parse_args():
    parser = argparse.ArgumentParser("Commandline arguments for running HunyuanImage-3 locally")
    # server
    parser.add_argument("--host", type=str, default="0.0.0.0", help="Host to run the server on")
    parser.add_argument("--port", type=int, default=8080, help="Port to run the server on")
    parser.add_argument("--image-cache-dir", type=str, help="Directory where images are saved.")
    # ui
    parser.add_argument("--open-sidebar", action="store_true", help="Whether to open the sidebar by default")
    # model
    parser.add_argument("--model-id", type=str, default="./HunyuanImage-3", help="Path to the model")
    parser.add_argument("--attn-impl", type=str, default="sdpa", choices=["sdpa", "flash_attention_2"],
                        help="Attention implementation")
    parser.add_argument("--moe-impl", type=str, default="eager", choices=["eager", "flashinfer"],
                        help="MoE implementation")
    # inference
    parser.add_argument("--seed", type=int, default="-1", help="Random seed")
    parser.add_argument("--diff-infer-steps", type=int, help="Number of inference steps")
    parser.add_argument("--diff-guidance-scale", type=float, help="Guidance scale")
    parser.add_argument("--image-size", type=str, default="auto", help="Image size")
    parser.add_argument("--bot-task", type=str, choices=["image", "auto", "think", "recaption", "img_ratio"],
                        help="Bot task type for generating text.")
    parser.add_argument("--context-mode", type=str, default="single_round", choices=["single_round", "unlimited"],
                        help="Context mode")
    parser.add_argument("--top-k", type=int, help="Top-K")
    parser.add_argument("--top-p", type=float, help="Top-P")
    parser.add_argument("--temperature", type=float, help="Temperature")
    parser.add_argument("--use-system-prompt", type=str,
                        choices=["en_vanilla", "en_recaption", "en_think_recaption", "dynamic", "custom", "None"],
                        help="System prompt type")
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    load_pipeline(args)

    chatbot_ui = create_ui_interface(args)
    chatbot_ui.launch(server_name=args.host, server_port=args.port, share=False)
